diff --git a/aimet_zoo_torch/xlsr/model/model_definition.py b/aimet_zoo_torch/xlsr/model/model_definition.py
index c1b446d..beee86e 100644
--- a/aimet_zoo_torch/xlsr/model/model_definition.py
+++ b/aimet_zoo_torch/xlsr/model/model_definition.py
@@ -14,9 +14,8 @@
 import json
 import os
 import torch
-from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
-from aimet_zoo_torch.common.downloader import Downloader
-from aimet_zoo_torch.common.super_resolution.models import XLSRRelease
+from utils.super_resolution.downloader import Downloader
+from utils.super_resolution.models import XLSRRelease
 
 
 class XLSR(XLSRRelease, Downloader):
@@ -77,40 +76,7 @@ class XLSR(XLSRRelease, Downloader):
             self.load_state_dict(state_dict)
             self.cuda()
         else:
-            state_dict = torch.load(self.path_pre_opt_weights)["state_dict"]
+            state_dict = torch.load(self.path_pre_opt_weights, map_location = torch.device('cpu'))["state_dict"]
             self.load_state_dict(state_dict)
-            self.cuda()
+            #self.cuda()
         self.eval()
-
-    def get_quantsim(self, quantized=False):
-        """get quantsim object with pre-loaded encodings"""
-        if not self.cfg:
-            raise NotImplementedError(
-                "There is no Quantization Simulation available for the model_config passed"
-            )
-        if quantized:
-            self.from_pretrained(quantized=True)
-        else:
-            self.from_pretrained(quantized=False)
-        device = torch.device("cuda")
-        dummy_input = torch.rand(self.input_shape, device=device)
-        kwargs = {
-            "quant_scheme": self.cfg["optimization_config"][
-                "quantization_configuration"
-            ]["quant_scheme"],
-            "default_param_bw": self.cfg["optimization_config"][
-                "quantization_configuration"
-            ]["param_bw"],
-            "default_output_bw": self.cfg["optimization_config"][
-                "quantization_configuration"
-            ]["output_bw"],
-            "config_file": self.path_aimet_config,
-            "dummy_input": dummy_input,
-        }
-        sim = QuantizationSimModel(self, **kwargs)
-        if self.path_aimet_encodings and quantized:
-            load_encodings_to_sim(sim, self.path_aimet_encodings)
-        if self.path_adaround_encodings and quantized:
-            sim.set_and_freeze_param_encodings(self.path_adaround_encodings)
-        sim.model.eval()
-        return sim
