{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33e1ee76-7229-4154-8f68-8ec75ba5edd9",
   "metadata": {},
   "source": [
    "# Setting UP SNPE SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e0ed81-13cb-4ad7-a14a-a9b842da8765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['QNN_SDK_ROOT']=\"/local/mnt/workspace/gokul/QNN/2.19.0.240124\" #set up your qnn path here.\n",
    "os.environ['ANDROID_NDK_ROOT']=\"/local/mnt/workspace/gokul/android-ndk-r25c\"\n",
    "os.environ['RAW_FILE_FOLDER']=\"input/raw\"\n",
    "os.environ['MODEL_NAME']=\"fcn_resnet101_fp32\"\n",
    "os.environ['QUANTIZED_MODEL_NAME']=\"fcn_resnet101_quant16_w8a16\"\n",
    "os.environ['MODEL_PATH']=\"models/fcn_resnet101_fp32\"\n",
    "os.environ['QUANTIZED_MODEL_PATH']=\"models/fcn_resnet101_quant16_w8a16\"\n",
    "os.environ['TARGET_INPUT_LIST']=\"input/input.txt\"\n",
    "os.environ['ONDEVICE_FOLDER']=\"fcn_resnet101\"\n",
    "os.environ['DEVICE_HOST']=\"localhost\"\n",
    "os.environ['DEVICE_ID']=\"bc468c1d\" #fill your device-id. Use command \"adb devices\" to get devices names. example :\"e18d5d0\"\n",
    "os.environ['QNN_TARGET_ARCH']=\"aarch64-android\"\n",
    "os.environ['QNN_TARGET_STL']=\"libQnnHtp.so\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea09199c-b80d-4f3d-a277-31625e4b987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision.models.segmentation as models\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb6dbe-8a77-4826-9d44-c25f09362bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = models.fcn_resnet101(pretrained=True)\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self,pretrained_model):\n",
    "        super(CustomModel,self).__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.argmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self,x):\n",
    "        output_dict = self.pretrained_model(x)\n",
    "        output = output_dict['out']\n",
    "        argmax_output = torch.argmax(output,dim=1,keepdim=False)\n",
    "        return argmax_output\n",
    "model = CustomModel(pretrained_model)\n",
    "input = torch.randn(1,3,400,400)\n",
    "output = model(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c13449-3ee0-413e-a424-2640d81ca997",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d4180-e2ef-496d-8d23-e875ca8a3fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1,3, 400, 400).type(torch.FloatTensor).to('cpu')\n",
    "torch.onnx.export(model, dummy_input, \"./models/fcn_resnet101.onnx\",opset_version=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0f7170-1db0-4597-841e-6506547adabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07173ba-596b-492a-a5cc-c258348c6fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('input',exist_ok=True)\n",
    "os.makedirs('input/dataset',exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c35fb-5c8b-4e26-9899-2247de1bd3aa",
   "metadata": {},
   "source": [
    "## Download dataset in input/dataset/ directory\n",
    "#### Try in terminal if notebook getting freezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2f78f-d90c-428c-a65f-27f8b6773bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd input/dataset/\n",
    "wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
    "tar -xvf VOCtrainval_11-May-2012.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9365fb-86b7-406b-9bae-406d2d909ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "url = 'http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar'\n",
    "response = requests.get(url,stream=True)\n",
    "size = int(response.headers.get('content-length',0))\n",
    "with open('input/dataset/VOCtrainval_11-May-2012.tar','wb') as file:\n",
    "    for data in tqdm(response.iter_content(chunk_size=1024), total=size, unit='B', unit_scale=True):\n",
    "        file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc86688-212a-4a0e-a988-bf78c4bb3872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "# Define the path to the large file\n",
    "file_path = \"input/dataset/VOCtrainval_11-May-2012.tar\"\n",
    "with tarfile.open(file_path, \"r:tar\") as tar:\n",
    "    file_size = sum([member.size for member in tar.getmembers()])   \n",
    "    progress_bar = tqdm(total=file_size, unit=\"B\", unit_scale=True) \n",
    "    for member in tar.getmembers():\n",
    "        tar.extract(member)\n",
    "        progress_bar.update(member.size)  \n",
    "    progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007c709-7bce-45d9-bb28-748a2138bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp -r VOCdevkit/ input/dataset/\n",
    "rm -rf VOCdevkit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d02dc-94dc-4367-b732-4dc325abc597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(file):\n",
    "    img = Image.open(file).convert('RGB')\n",
    "    img = transform(img).unsqueeze(0) # To tensor of NCHW\n",
    "    # print(img.shape)\n",
    "    img = np.transpose(img, (0,2,3,1))\n",
    "    #print(img.shape)  \n",
    "    img = img.numpy()\n",
    "    print(img.shape)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c72ce1b-8c60-464a-8bf6-ff85d206b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('input/raw',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db64d03-9ebf-48b3-935f-b67ea489f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_dir = \"input/dataset/VOCdevkit/VOC2012/JPEGImages/\"\n",
    "image_count=15\n",
    "\n",
    "transform = T.Compose([\n",
    "                T.Resize((400,400)),\n",
    "                \n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "image_paths = []\n",
    "c=0\n",
    "inputlist = open('input/dataset/VOCdevkit/VOC2012/ImageSets/Segmentation/val.txt', 'r')\n",
    "Lines = inputlist.readlines()\n",
    "Lines = [line.strip() + \".jpg\" for line in Lines]\n",
    "for line in Lines:\n",
    "    image_paths.extend(glob.glob(os.path.join(test_images_dir, line)))\n",
    "    c = c+1\n",
    "    if(c==image_count):\n",
    "        break\n",
    "image_paths = sorted(image_paths)\n",
    "count=0\n",
    "for img_path in image_paths:\n",
    "    print(\"img_path: \", img_path)\n",
    "    img = preProcess(img_path)\n",
    "    name = img_path.replace(test_images_dir,\"\")\n",
    "    name = name.split('.')[0]\n",
    "    #print(name)\n",
    "    raw_data = os.path.join('input/raw/preproc_'+name+'.raw')\n",
    "    img.tofile(raw_data)\n",
    "    count = count+1\n",
    "    if count==image_count:\n",
    "        break\n",
    "    print(\"raw image saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34cac77-80a9-4df8-8fe5-d4dcff12fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'input/raw/'\n",
    "output_file_path = 'input/input.txt'  # The file where the output will be saved\n",
    "all_files = os.listdir(directory_path)\n",
    "# Filter only the .raw files and create a list of their names\n",
    "raw_files = [file for file in all_files if file.endswith('.raw')]\n",
    "raw_files = sorted(raw_files)\n",
    "# Write the file names to the output file\n",
    "with open(output_file_path, 'w') as f:\n",
    "    c=0\n",
    "    for raw_file in raw_files:\n",
    "        f.write(f\"./raw/{raw_file}\\n\")\n",
    "        c=c+1\n",
    "print(f\"File names written to {output_file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c383a5-f64d-44c3-8208-a1d5b65e013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ${QNN_SDK_ROOT}/bin/envsetup.sh\n",
    "export PATH=${ANDROID_NDK_ROOT}:${PATH}\n",
    "export TMPDIR=${PWD}/temp\n",
    "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-onnx-converter --input_network models/fcn_resnet101.onnx --output_path ${MODEL_PATH}.cpp\n",
    "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator -c ${MODEL_PATH}.cpp -b ${MODEL_PATH}.bin -o models/model_libs\n",
    "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-context-binary-generator \\\n",
    "              --backend ${QNN_SDK_ROOT}/lib/x86_64-linux-clang/libQnnHtp.so \\\n",
    "              --model models/model_libs/x86_64-linux-clang/lib${MODEL_NAME}.so \\\n",
    "              --binary_file ${MODEL_NAME}.serialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58792696-c7c2-419e-9bb3-56e5213a47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source ${QNN_SDK_ROOT}/bin/envsetup.sh\n",
    "export PATH=${ANDROID_NDK_ROOT}:${PATH}\n",
    "export TMPDIR=${PWD}/temp\n",
    "cd input\n",
    "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-onnx-converter --input_network ../models/fcn_resnet101.onnx --output_path ../${QUANTIZED_MODEL_PATH}.cpp --input_list input.txt \\\n",
    "--param_quantizer \"adjusted\" --act_quantizer \"enhanced\"\n",
    "cd ..\n",
    "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-model-lib-generator -c ${QUANTIZED_MODEL_PATH}.cpp -b ${QUANTIZED_MODEL_PATH}.bin -o models/model_libs2\n",
    "${QNN_SDK_ROOT}/bin/x86_64-linux-clang/qnn-context-binary-generator \\\n",
    "              --backend ${QNN_SDK_ROOT}/lib/x86_64-linux-clang/libQnnHtp.so \\\n",
    "              --model models/model_libs2/x86_64-linux-clang/lib${QUANTIZED_MODEL_NAME}.so \\\n",
    "              --binary_file ${QUANTIZED_MODEL_NAME}.serialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fedfcf-9c0d-4021-91fa-996e7507313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export DEVICE_SHELL=\"adb -H $DEVICE_HOST -s $DEVICE_ID\"\n",
    "$DEVICE_SHELL shell \"mkdir -p /data/local/tmp/qnnexample/$QNN_TARGET_ARCH/bin\" && $DEVICE_SHELL shell \"mkdir -p /data/local/tmp/qnnexample/$QNN_TARGET_ARCH/lib\" && $DEVICE_SHELL shell \"mkdir -p /data/local/tmp/qnnexample/dsp/lib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a9abe-c0b8-4982-872b-1713b1d9200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export DEVICE_SHELL=\"adb -H $DEVICE_HOST -s $DEVICE_ID\"\n",
    "$DEVICE_SHELL shell \"mkdir -p /data/local/tmp/$ONDEVICE_FOLDER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0005f2-de81-42c5-bfa5-3c2f607ec761",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export DEVICE_SHELL=\"adb -H $DEVICE_HOST -s $DEVICE_ID\"\n",
    "$DEVICE_SHELL push $QNN_SDK_ROOT/lib/$QNN_TARGET_ARCH/$QNN_TARGET_STL /data/local/tmp/$ONDEVICE_FOLDER\n",
    "$DEVICE_SHELL push $QNN_SDK_ROOT/bin/$QNN_TARGET_ARCH/qnn-net-run /data/local/tmp/$ONDEVICE_FOLDER\n",
    "$DEVICE_SHELL push $QNN_SDK_ROOT/lib/hexagon-v73/unsigned/*.so /data/local/tmp/$ONDEVICE_FOLDER\n",
    "$DEVICE_SHELL push $QNN_SDK_ROOT/lib/$QNN_TARGET_ARCH/*.so /data/local/tmp/$ONDEVICE_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c10d4-ef2d-43d1-adbb-6418f77a47db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#find ./raw -name *.raw > list.txt\n",
    "export DEVICE_SHELL=\"adb -H $DEVICE_HOST -s $DEVICE_ID\"\n",
    "$DEVICE_SHELL push output/${QUANTIZED_MODEL_NAME}.serialized.bin /data/local/tmp/$ONDEVICE_FOLDER\n",
    "$DEVICE_SHELL push output/${MODEL_NAME}.serialized.bin /data/local/tmp/$ONDEVICE_FOLDER\n",
    "$DEVICE_SHELL push models/model_libs2/aarch64-android/lib${QUANTIZED_MODEL_NAME}.so /data/local/tmp/$ONDEVICE_FOLDER\n",
    "$DEVICE_SHELL push $RAW_FILE_FOLDER /data/local/tmp/$ONDEVICE_FOLDER\n",
    "$DEVICE_SHELL push $TARGET_INPUT_LIST /data/local/tmp/$ONDEVICE_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d07758-dd3b-45e7-ae57-02eb06110537",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "export DEVICE_SHELL=\"adb -H $DEVICE_HOST -s $DEVICE_ID\"\n",
    "$DEVICE_SHELL shell\n",
    "export ONDEVICE_FOLDER=\"fcn_resnet101\"\n",
    "export device_path=/data/local/tmp/$ONDEVICE_FOLDER\n",
    "export LD_LIBRARY_PATH=$device_path\n",
    "export ADSP_LIBRARY_PATH=\"$device_path\"\n",
    "export PATH=$PATH:$device_path\n",
    "export QUANTIZED_MODEL_NAME=\"fcn_resnet101_quant16_w8a16\"\n",
    "cd $device_path\n",
    "export OUTPUT_FOLDER=OUTPUT_Quant_DSP\n",
    "cd /data/local/tmp/$ONDEVICE_FOLDER  && \n",
    "./qnn-net-run --backend libQnnHtp.so --input_list input.txt --retrieve_context ${QUANTIZED_MODEL_NAME}.serialized.bin --output_dir $OUTPUT_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ce847-db91-4282-bb0d-1bd1819cbd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export DEVICE_SHELL=\"adb -H $DEVICE_HOST -s $DEVICE_ID\"\n",
    "$DEVICE_SHELL shell\n",
    "export ONDEVICE_FOLDER=\"fcn_resnet101\"\n",
    "export device_path=/data/local/tmp/$ONDEVICE_FOLDER\n",
    "export LD_LIBRARY_PATH=$device_path\n",
    "export ADSP_LIBRARY_PATH=\"$device_path\"\n",
    "export PATH=$PATH:$device_path\n",
    "export QUANTIZED_MODEL_NAME=\"fcn_resnet101_quant16_w8a16\"\n",
    "cd $device_path\n",
    "export OUTPUT_FOLDER=OUTPUT_CPU\n",
    "cd /data/local/tmp/$ONDEVICE_FOLDER  && \n",
    "./qnn-net-run --backend libQnnCpu.so --input_list input.txt --model lib${QUANTIZED_MODEL_NAME}.so --output_dir $OUTPUT_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed45d308-20d9-473a-9b5c-e30fa926eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa3e118-c9e1-426b-9dcd-4524168c6003",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf OUTPUT_Quant_DSP\n",
    "rm -rf OUTPUT_CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247650d9-d56e-497a-a665-03d94c86ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export DEVICE_SHELL=\"adb -H $DEVICE_HOST -s $DEVICE_ID\"\n",
    "$DEVICE_SHELL pull /data/local/tmp/$ONDEVICE_FOLDER/OUTPUT_Quant_DSP output/OUTPUT_Quant_DSP\n",
    "$DEVICE_SHELL pull /data/local/tmp/$ONDEVICE_FOLDER/OUTPUT_CPU output/OUTPUT_CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f4305-618f-4bf6-8478-1439beb79e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_overlay(image, segmented_image):\n",
    "    alpha = 1 # transparency for the original image\n",
    "    beta = 0.8 # transparency for the segmentation map\n",
    "    gamma = 0 # scalar added to each sum\n",
    "    # print(image.size)\n",
    "    # print(segmented_image.shape)\n",
    "    segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR)\n",
    "    image = np.array(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.addWeighted(image, alpha, segmented_image, beta, gamma, image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6344c5b3-2523-4889-8086-dd6ee81c83ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "crop_size = 400\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((crop_size,crop_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383f20d-3197-4658-83d1-5c30550a9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = [\n",
    "               (0, 0, 0),  # background\n",
    "               (128, 0, 0), # aeroplane\n",
    "               (0, 128, 0), # bicycle\n",
    "               (128, 128, 0), # bird\n",
    "               (0, 0, 128), # boat\n",
    "               (128, 0, 128), # bottle\n",
    "               (0, 128, 128), # bus \n",
    "               (128, 128, 128), # car\n",
    "               (64, 0, 0), # cat\n",
    "               (192, 0, 0), # chair\n",
    "               (64, 128, 0), # cow\n",
    "               (192, 128, 0), # dining table\n",
    "               (64, 0, 128), # dog\n",
    "               (192, 0, 128), # horse\n",
    "               (64, 128, 128), # motorbike\n",
    "               (192, 128, 128), # person\n",
    "               (0, 64, 0), # potted plant\n",
    "               (128, 64, 0), # sheep\n",
    "               (0, 192, 0), # sofa\n",
    "               (128, 192, 0), # train\n",
    "               (0, 64, 128) # tv/monitor\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e32bc-8a32-4eb5-ad3b-ff3e4a45c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segment_labels(image, model, device):\n",
    "    # transform the image to tensor and load into computation device\n",
    "    image = transform(image).to(device)\n",
    "    image = image.unsqueeze(0) # add a batch dimension\n",
    "    outputs = model(image)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c6fbbc-fe9b-46f1-a09d-66677b658c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_segmentation_map(outputs):\n",
    "    labels = outputs.detach().cpu().numpy()\n",
    "    # create Numpy arrays containing zeros\n",
    "    # later to be used to fill them with respective red, green, and blue pixels\n",
    "    red_map = np.zeros_like(labels).astype(np.uint8)\n",
    "    green_map = np.zeros_like(labels).astype(np.uint8)\n",
    "    blue_map = np.zeros_like(labels).astype(np.uint8)\n",
    "    \n",
    "    for label_num in range(0, len(label_map)):\n",
    "        index = labels == label_num\n",
    "        red_map[index] = np.array(label_map)[label_num, 0]\n",
    "        green_map[index] = np.array(label_map)[label_num, 1]\n",
    "        blue_map[index] = np.array(label_map)[label_num, 2]\n",
    "        \n",
    "    segmentation_map = np.stack([red_map, green_map, blue_map], axis=2)\n",
    "    return segmentation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c52e4b-25bc-4023-8719-1ef497c0d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageNames():\n",
    "    inputlist = open('input/input.txt', 'r')\n",
    "    Lines = inputlist.readlines()\n",
    "    count = 0\n",
    "    imageList = []\n",
    "    for line in Lines:\n",
    "        name = line.split(\"preproc_\",1)[1]\n",
    "        name = name.split('.')[0]\n",
    "        imageList.append(name)\n",
    "        count += 1\n",
    "    return imageList\n",
    "imageList = ImageNames()\n",
    "print((imageList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba800e6-d036-4889-b5f1-917a34a2c642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "device = torch.device('cpu')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e40e33e-e46b-4291-b1b7-8f9ae603b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('output/model_prediction', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad558bd-54b8-4e57-82fb-cafc49da4fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'input/dataset/VOCdevkit/VOC2012/JPEGImages/'\n",
    "from PIL import Image\n",
    "import cv2\n",
    "for i in range(0,len(imageList)):\n",
    "\n",
    "    image = Image.open(image_dir+imageList[i]+'.jpg')\n",
    "    # do forward pass and get the output dictionary\n",
    "    image  = image.resize((crop_size,crop_size))\n",
    "    # print(image.size)\n",
    "    outputs = get_segment_labels(image, model, device)\n",
    "    # get the data from the `out` key\n",
    "    # outputs = outputs['out']\n",
    "    # print(type(outputs))\n",
    "    # print(outputs.shape)\n",
    "    segmented_image = draw_segmentation_map(outputs[0])\n",
    "    print(image.size)\n",
    "    print(segmented_image.shape)\n",
    "    final_image = image_overlay(image, segmented_image)\n",
    "    # show the segmented image and save to disk\n",
    "    # cv2.imshow('Segmented image', final_image)\n",
    "    # cv2.waitKey(0)\n",
    "    cv2.imwrite(f\"output/model_prediction/{imageList[i]}.jpg\", final_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe377f7-b9a8-4d48-8a3e-4a4c0b537edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PostProc(img_path, out_path,i):\n",
    "    res = np.fromfile(img_path, dtype=\"float32\")\n",
    "    res_reshape = res.reshape((1,400,400)).astype(np.float32)\n",
    "    model_img = torch.from_numpy(res_reshape)\n",
    "    segmented_image = draw_segmentation_map(model_img[0])\n",
    "    image = Image.open(image_dir+imageList[i]+'.jpg')\n",
    "    # do forward pass and get the output dictionary\n",
    "    image  = image.resize((crop_size,crop_size))\n",
    "    final_image = image_overlay(image, segmented_image)\n",
    "    cv2.imwrite(f\"{out_path}\", final_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d407324-addc-4d5b-b4c6-601b4f155a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('output/test_results', exist_ok=True)\n",
    "os.makedirs('output/test_results/32b_arm', exist_ok=True)\n",
    "os.makedirs('output/test_results/8b_dsp', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d05b3-dd99-4a58-8c11-108b2c58abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_dir = \"output/OUTPUT_32b_CPU/\"\n",
    "image_dir = 'input/dataset/VOCdevkit/VOC2012/JPEGImages/'\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "for i in range(0,len(imageList)):\n",
    "    img_path = os.path.join(test_images_dir, 'Result_')\n",
    "    img_path = img_path+str(i)+'/1002.raw'\n",
    "    out_path = 'output/test_results/32b_arm/'+imageList[i]+'_prediction_32b_arm.png'\n",
    "    PostProc(img_path, out_path,i)\n",
    "    i = i +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0cf6d4-841d-4ed7-bfce-4b3b75be0bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_dir = \"output/OUTPUT_8b_DSP/\"\n",
    "for i in range(0,len(imageList)):\n",
    "    img_path = os.path.join(test_images_dir, 'Result_')\n",
    "    img_path = img_path+str(i)+'/1002.raw'\n",
    "    out_path = 'output/test_results/8b_dsp/'+imageList[i]+'_prediction_8b_dsp.png'\n",
    "    PostProc(img_path, out_path,i)\n",
    "    i = i +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467eb305-e30a-4792-8d04-518cdf7cb6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(30, 100));\n",
    "import cv2\n",
    "for i in range(0, 5):\n",
    "    \n",
    "    original = cv2.imread('input/dataset/VOCdevkit/VOC2012/JPEGImages/'+imageList[i]+'.jpg')\n",
    "    original = cv2.resize(original, (400,400))\n",
    "    original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "    ax = fig.add_subplot(28,4,4*i+1);\n",
    "    plt.imshow(original,cmap='gray');\n",
    "    ax.set_title('original image\\n');\n",
    "    ax.axis('off');\n",
    "    \n",
    "    pth_inf = cv2.imread('output/model_prediction/'+imageList[i]+'.jpg')\n",
    "    pth_inf = cv2.resize(pth_inf, (400,400))\n",
    "    # pth_inf = cv2.cvtColor(pth_inf, cv2.COLOR_BGR2RGB)\n",
    "    # pth_overlay = image_overlay(original, pth_inf)\n",
    "    ax = fig.add_subplot(28,4,4*i+2);\n",
    "    plt.imshow(pth_inf,cmap='gray');\n",
    "    ax.set_title('pth output\\n');\n",
    "    ax.axis('off');\n",
    "\n",
    "\n",
    "    arm_fp32= cv2.imread('output/test_results/32b_arm/'+imageList[i]+'_prediction_32b_arm.png')\n",
    "    # arm_fp32 = cv2.cvtColor(arm_fp32, cv2.COLOR_BGR2RGB)\n",
    "    # fp32_overlay = image_overlay(original, arm_fp32)\n",
    "    arm_fp32 = cv2.resize(arm_fp32, (400,400))\n",
    "    ax = fig.add_subplot(28,4,4*i+3);\n",
    "    plt.imshow(arm_fp32,cmap='gray');\n",
    "    ax.set_title('fp32 on ARM\\n');\n",
    "    ax.axis('off');\n",
    "\n",
    "    dsp= cv2.imread('output/test_results/8b_dsp/'+imageList[i]+'_prediction_8b_dsp.png')\n",
    "    # dsp_int8 = cv2.cvtColor(dsp_int8, cv2.COLOR_BGR2RGB)\n",
    "    # int8_overlay = image_overlay(original, dsp_int8)\n",
    "    # print(dsp)\n",
    "    ax = fig.add_subplot(28,4,4*i+4);\n",
    "    plt.imshow(dsp,cmap='gray');\n",
    "    ax.set_title('int8 on DSP\\n');\n",
    "    ax.axis('off');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
